{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "389e575f",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6bb44c27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T19:56:55.094705Z",
     "start_time": "2022-10-25T19:56:55.013898Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "tokenizers.__version__: 0.13.1\n",
      "transformers.__version__: 4.23.1\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "import tokenizers\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", 4000) # to not cut off text in dataframes\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "font = {'size': 14}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "235444b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T20:41:03.795638Z",
     "start_time": "2022-10-25T20:41:03.670796Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3911, 8)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/train.csv.zip')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "target_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd503df8",
   "metadata": {},
   "source": [
    "# Try out GPT3 to solve the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc6fa5ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T19:57:05.477303Z",
     "start_time": "2022-10-25T19:57:05.438304Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install openai\n",
    "import json\n",
    "import openai\n",
    "openai.api_key = json.load(open('openai_key.json', 'r'))[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "152eb49c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T19:57:05.957006Z",
     "start_time": "2022-10-25T19:57:05.592169Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curie-similarity\n",
      "code-search-babbage-text-001\n",
      "babbage-search-query\n",
      "text-davinci-001\n",
      "davinci-search-query\n",
      "text-search-babbage-query-001\n",
      "text-search-curie-query-001\n",
      "davinci-instruct-beta\n",
      "text-curie-001\n",
      "code-search-babbage-code-001\n"
     ]
    }
   ],
   "source": [
    "for model in openai.Model.list()[\"data\"][:10]:\n",
    "    print(model[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84591dd9",
   "metadata": {},
   "source": [
    "From https://beta.openai.com/docs/models/overview and https://openai.com/api/pricing/\n",
    "- text-davinci-002: Most capable, max 4096 tokens, training data up to Jun 2021 (\\$0.02 per 1k tokens)\n",
    "- text-curie-001: Very capable, faster, lower cost, max 2048 tokens, training data up to Oct 2019 (\\$0.002 per 1k tokens)\n",
    "- text-babbage-001: Capable, very fast, low cost, max 2048 tokens, training data up to Oct 2019 (\\$0.0005 per 1k tokens)\n",
    "- text-ada-001: For simple tasks, fastest, lowest cost, max 2048 tokens, training data up to Oct 2019 (\\$0.0004 per 1k tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7146cd33",
   "metadata": {},
   "source": [
    "## Try to teach GPT3 the feedback price task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "df36b84c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T20:41:11.606407Z",
     "start_time": "2022-10-25T20:41:11.566511Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"length\"] = df[\"full_text\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d0e9ada9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T20:41:11.902747Z",
     "start_time": "2022-10-25T20:41:11.786276Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"length\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "01b312ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T20:41:12.149855Z",
     "start_time": "2022-10-25T20:41:12.115584Z"
    }
   },
   "outputs": [],
   "source": [
    "target_col = \"grammar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2b14198d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T20:41:13.255049Z",
     "start_time": "2022-10-25T20:41:13.214656Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0    994\n",
       "3.5    880\n",
       "2.5    855\n",
       "2.0    544\n",
       "4.0    447\n",
       "4.5    134\n",
       "5.0     29\n",
       "1.5     20\n",
       "1.0      8\n",
       "Name: grammar, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[target_col].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "daa6bd64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T20:41:13.811679Z",
     "start_time": "2022-10-25T20:41:13.768919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(572, 9)\n",
      "(855, 9)\n",
      "(994, 9)\n",
      "(1327, 9)\n",
      "(163, 9)\n"
     ]
    }
   ],
   "source": [
    "print(df[df[target_col]<=2].shape)\n",
    "print(df[(df[target_col] > 2.0) & (df[target_col] <= 2.5)].shape)\n",
    "print(df[(df[target_col] > 2.5) & (df[target_col] <= 3.0)].shape)\n",
    "print(df[(df[target_col] > 3.0) & (df[target_col] <= 4.0)].shape)\n",
    "print(df[(df[target_col] > 4.0) & (df[target_col] <= 6.0)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "de661386",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T20:41:16.038836Z",
     "start_time": "2022-10-25T20:41:15.993336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test shape: (13, 9)\n"
     ]
    }
   ],
   "source": [
    "target_col = \"grammar\"\n",
    "text_col = \"short_text\"\n",
    "start_seed = 3222\n",
    "nb_test_examples_rare_categories = 2\n",
    "\n",
    "# 10 examples from all ranges of scores to test\n",
    "df_test = pd.concat([\n",
    "    df[df[target_col] <= 1.5].sample(nb_test_examples_rare_categories, random_state=start_seed+6), \n",
    "    df[(df[target_col] > 1.5) & (df[target_col] <= 2.5)].sample(nb_test_examples_rare_categories+1, random_state=start_seed+7), \n",
    "    df[(df[target_col] > 2.5) & (df[target_col] <= 3.0)].sample(nb_test_examples_rare_categories+1, random_state=start_seed+8), \n",
    "    df[(df[target_col] > 3.0) & (df[target_col] <= 4.0)].sample(nb_test_examples_rare_categories+1, random_state=start_seed+9), \n",
    "    df[df[target_col] > 4.0].sample(nb_test_examples_rare_categories, random_state=start_seed+10), \n",
    "])\n",
    "df_test = df_test.sample(len(df_test), random_state=start_seed+11)\n",
    "print(f\"Test shape: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4dbcba12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T20:42:05.362708Z",
     "start_time": "2022-10-25T20:42:05.312177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (8, 10)\n",
      "Decide whether an essay's grammar is very bad, bad, okay, good or very good. Also, explain your decision.\n",
      "\n",
      "Essay: Firstly, Adopting is good for teenagers and may help keep them out of trouble and even breaking the law and others things , adopting may help a lot to teenagers to have a home and education a life better and thats what they need is parent the they support them with everything the they could do for them to help, parents are the best thing the we have in our lives also, teenagers on the street think about why I could have a better life or home it and is so sad for them the they could it have for them seen other teenager having all the good things with their parents, kids on street dont realize the they can get on trouble because they didnt have any education even they dont think because of how they live on the street breaking the laws there a\n",
      "Grammar: bad\n",
      "\n",
      "--\n",
      "\n",
      "Essay: Good actions can be helpful in most ways. Good altered can led to good grades,goal,and relshenship. If you do all of these thing you will be good in your whole life. People often do not use thins things in there life but they should use it. This are easel way to do it you can do it and any you can do it.\n",
      "\n",
      "To be secssful you have good grades two of my friends. They name of my two friends are Generic_Name and Generic_Name. Generic_Name had good grades for all of her 4 years in middle school .Generic_Name is in the a roll every semester for the 4 semesters. Generic_Name trues so hard in all of the school work and she did not care about her friends. Generic_Name learned how to do math,reading, and writhing. She prates every time she had a test \n",
      "Grammar: good\n",
      "\n",
      "--\n",
      "\n",
      "Essay: The Primer Minister Winston Churrchill once said that \"success consists of going from failure to failure without loss of enthusiasm.\" And the question is about angree or desag.\n",
      "\n",
      "My opinion is desagree because the Primer Minister wiston not is try to motivate to onther person to do again. First,reason when something person loss manytime something and feel what if continue does again can obtain something good in the future. Because never said end. Second, reason the failure plays inthe life a paper strong in the life of every person in the world. But when any person not try more time that one is clear that is going to failure and never feel what is obtain something good in the futured. The decition every person have nothing can said what try \n",
      "Grammar: very bad\n",
      "\n",
      "--\n",
      "\n",
      "Essay: When you have a problem do you think to yourself, how can I do my best to fix this problem? American jazz legend Duke Ellington said,\"A problem is a chance for you to do your best.\" In my opinion, I agree with Duke Ellington's quote. One reason, is if you don't try to do your best in a situation you could end up regretting your decision. Another reason, is if you try do your best in a situation you could end up learning something new. Lastly, if you don't put any effort try to resolve a problem you could end up back in that hole.\n",
      "\n",
      "I agree with this quote because if you don't try to do your best in a situation you could end up regretting your decision. One reason, is the amount effort you put in to a problem could affect your future. Also, a\n",
      "Grammar: very good\n",
      "\n",
      "--\n",
      "\n",
      "Essay: I think i'm going to agree what Churchill had said. Because, if i success so easily, i won't able to understand how hard to earn something. Some people think they are smart and they'd never fail. But me, i thought a different way. I thought that if i fail,i 'll have more experience. For an example, in my country, Vietnam, people always say that failure is the key lead to your success. I have failed many times in my life, but i never give up. I remembered when i was 8, i was really love the egg that my mom made for me. So i asked her how to made that egg, and of course i had broken a lot of eggs. And i also wanted to give up, but, i'm really love the foods that made from egg. So i continued learning how to cook the egg, and now i succeed wit\n",
      "Grammar: bad\n",
      "\n",
      "--\n",
      "\n",
      "Essay: To begin, what are your thoughts on first impressions through peoples point of view? When two people interact with one another the impression is not always dependable because People are probably not themselves, People opinion change, People never have the same intellectual connection. For example Generic_Name went to the movies with his girlfriend Generic_Name, his mom Generic_Name, and his sister Generic_Name. Generic_Name tells Generic_Name to invite her boyfriend Generic_Name. Generic_Name interaction with Generic_Name was unpleasant, based on what Generic_Name saw their was a different perspective from one an other. Days later at night the family reunites and the perspective change from what Generic_Name thought at first and what she th\n",
      "Grammar: okay\n",
      "\n",
      "--\n",
      "\n",
      "Essay: Hi, my name is Generic_Name. I heard that you are accepting people to come look at some businesses they prefer, and learn more about what they are interested in. One thing I'm interested in knowing is, How many people do you let come and experience this opportunity ? I strongly feel like it would be a pleasure to be able to come and see the environment or being able to understand the right and wrong ways of teaching kids. I think I would be great for this position because I am a well rounded person, very respectful, and responsible.\n",
      "\n",
      "I feel like being a well rounded person is a good character trait everyone should have. I could demonstrate that in many ways. You are probably thinking, \"Generic_Name, everyone can be a well rounded person in \n",
      "Grammar: good\n",
      "\n",
      "--\n",
      "\n",
      "Essay: I want to work as a animal babysitter, I had a lot of experience of taking care of animals I learned how to take care of animals and fishes, everyday I feed my guinea pigs and fishes and my dog, I have taken care of animals since was little, since one day I want to visit a pet Daycare and explore the daycare of how dogs are being taken care of, I want to learn about how to take care of new animals, I want to rescue animals in Mexico, cause I'll save animals in Mexico since I have a ranch there, I could care for the dogs or other animals, since I saved lives of them when I was in Mexico, I would feed them or help them from abuse from other people, I had experience for caring for animals, since I want to learn the correct way of taking care o\n",
      "Grammar: okay\n",
      "\n",
      "--\n",
      "\n",
      "Essay: What lesson would you chose to teach elementary school students? Would you chose math because they like numbers, or science because they like playing with science experiments. I would chose science, because we use science every day by turning on a light, to throwing a ball up in the air. Another reason many students like science is because it is the most fun, by that I mean by all the experiments we do in science. One lesson I would like to teach elementary school students is about science because it is fun, it is the most easiest subject to teach, and it is the most used in daily life.\n",
      "\n",
      "Firstly, one lesson I would like to teach is science because it is fun to most adults and kids. First, science is fun because you get to do different types\n",
      "\n",
      "Grammar:\n"
     ]
    }
   ],
   "source": [
    "gpt_model_type = \"text-curie-001\"\n",
    "nb_train_examples_rare_categories = 1\n",
    "max_length_train_examples = 750\n",
    "\n",
    "# gpt_model_type = \"text-davinci-002\"\n",
    "# nb_train_examples_rare_categories = 1\n",
    "# max_length_train_examples = 1500\n",
    "# nb_train_examples_rare_categories = 2\n",
    "# max_length_train_examples = 1000\n",
    "\n",
    "df[\"short_text\"] = df[\"full_text\"].apply(lambda x: x[:max_length_train_examples])\n",
    "df_test[\"short_text\"] = df_test[\"full_text\"].apply(lambda x: x[:max_length_train_examples])\n",
    "score_to_label = {1.0: \"very bad\", 1.5: \"very bad\", 2.0: \"bad\", 2.5: \"bad\", 3.0: \"okay\", 3.5: \"good\", 4.0: \"good\", 4.5: \"very good\", 5.0: \"very good\"}\n",
    "label_to_score = {'very bad': 1.5, 'bad': 2.5, 'okay': 3.0, 'good': 3.5, 'very good': 4.5}\n",
    "\n",
    "def render_train_example(essay, label):\n",
    "    return f\"\"\"Essay: {essay}\\n{target_col.capitalize()}: {score_to_label[label]}\"\"\"\n",
    "    \n",
    "def create_prompt(train_essays, train_labels, test_essay):\n",
    "    render_train_examples = \"\\n\\n--\\n\\n\".join(\n",
    "    [\n",
    "        render_train_example(essay, label) for essay, label in zip(train_essays, train_labels)\n",
    "    ])\n",
    "    return (f\"\"\"Decide whether an essay's {target_col} is very bad, bad, okay, good or very good. Also, explain your decision.\n",
    "    \\n\\n{render_train_examples}\\n\\n--\\n\\nEssay: {test_essay}\\n\\n{target_col.capitalize()}:\"\"\")\n",
    "\n",
    "# One example from each level of scoring to teach GPT3 how the task should be done + Don't use test examples\n",
    "df_train = pd.concat([\n",
    "    df[(~df.index.isin(df_test.index.tolist())) & (df[target_col] <= 1.5)].sample(nb_train_examples_rare_categories, random_state=start_seed), \n",
    "    df[(~df.index.isin(df_test.index.tolist())) & (df[target_col] > 1.5) & (df[target_col] <= 2.5)].sample(\n",
    "        nb_train_examples_rare_categories+1, random_state=start_seed+1), \n",
    "    df[(~df.index.isin(df_test.index.tolist())) & (df[target_col] > 2.5) & (df[target_col] <= 3.0)].sample(\n",
    "        nb_train_examples_rare_categories+1, random_state=start_seed+2), \n",
    "    df[(~df.index.isin(df_test.index.tolist())) & (df[target_col] > 3.0) & (df[target_col] <= 4.0)].sample(\n",
    "        nb_train_examples_rare_categories+1, random_state=start_seed+3), \n",
    "    df[(~df.index.isin(df_test.index.tolist())) & (df[target_col] > 4.0)].sample(nb_train_examples_rare_categories, random_state=start_seed+4), \n",
    "])\n",
    "df_train = df_train.sample(len(df_train), random_state=start_seed+5) # shuffle the order\n",
    "print(f\"Train shape: {df_train.shape}\")\n",
    "\n",
    "print(create_prompt(df_train[text_col].tolist(), df_train[target_col].tolist(), df_test[text_col].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d4d5e8a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T20:42:37.478462Z",
     "start_time": "2022-10-25T20:42:36.987891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " good\n"
     ]
    }
   ],
   "source": [
    "gpt3_predictions = []\n",
    "for k in range(1):\n",
    "    output = openai.Completion.create(\n",
    "        model=gpt_model_type, \n",
    "        prompt=create_prompt(df_train[text_col].tolist(), df_train[target_col].tolist(), df_test[text_col].iloc[k]), \n",
    "        max_tokens=100\n",
    "    )\n",
    "    gpt3_predictions.append(output[\"choices\"][0][\"text\"])\n",
    "    print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4850a3e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-22T18:00:22.864388Z",
     "start_time": "2022-10-22T18:00:19.934791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " good\n",
      " bad\n",
      "\n",
      "\n",
      "very bad\n",
      " bad\n",
      " bad\n",
      " bad\n",
      " good\n",
      " good\n",
      " bad\n",
      " bad\n",
      "\n",
      "\n",
      "very bad\n",
      " good\n",
      " bad\n"
     ]
    }
   ],
   "source": [
    "gpt3_predictions = []\n",
    "for k in range(len(df_test)):\n",
    "    output = openai.Completion.create(\n",
    "        model=gpt_model_type, \n",
    "        prompt=create_prompt(df_train[text_col].tolist(), df_train[target_col].tolist(), df_test[text_col].iloc[k]), \n",
    "        max_tokens=6\n",
    "    )\n",
    "    gpt3_predictions.append(output[\"choices\"][0][\"text\"])\n",
    "    print(output[\"choices\"][0][\"text\"])\n",
    "\n",
    "# i = 0\n",
    "# for idx, row in df_test.iterrows():\n",
    "#     print(f\"{target_col} Prediction is {label_to_score[gpt3_predictions[i].lower().strip()]}, Truth is {row[target_col]}\")\n",
    "#     print(\"#\"*200)\n",
    "#     print(row[\"full_text\"])\n",
    "#     print(\"#\"*200)\n",
    "#     print(\"#\"*200)\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9882855",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-22T18:00:23.801498Z",
     "start_time": "2022-10-22T18:00:23.771123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grammar: avg. absolute error: 1.08 +/- 0.73\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpt3_grammar</th>\n",
       "      <th>grammar</th>\n",
       "      <th>absolute_error_grammar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426</th>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3192</th>\n",
       "      <td>1.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>2.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1357</th>\n",
       "      <td>2.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527</th>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3910</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2845</th>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2336</th>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2325</th>\n",
       "      <td>1.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558</th>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      gpt3_grammar  grammar  absolute_error_grammar\n",
       "807            3.5      3.5                     0.0\n",
       "3426           2.5      4.5                     2.0\n",
       "3192           1.5      3.5                     2.0\n",
       "699            2.5      3.5                     1.0\n",
       "1357           2.5      1.5                     1.0\n",
       "1527           2.5      3.0                     0.5\n",
       "3910           3.5      3.0                     0.5\n",
       "1937           3.5      1.5                     2.0\n",
       "2845           2.5      3.0                     0.5\n",
       "2336           2.5      2.5                     0.0\n",
       "2325           1.5      2.5                     1.0\n",
       "1558           3.5      2.0                     1.5\n",
       "1016           2.5      4.5                     2.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[f\"gpt3_{target_col}\"] = [label_to_score[k.lower().strip()] for k in gpt3_predictions] #gpt3_predictions\n",
    "df_test[f\"absolute_error_{target_col}\"] = df_test.apply(lambda row: abs(row[f\"gpt3_{target_col}\"] - row[target_col]), axis=1)\n",
    "print(f'{target_col}: avg. absolute error: {np.mean(df_test[f\"absolute_error_{target_col}\"]):.2f} +/- {np.std(df_test[f\"absolute_error_{target_col}\"]):.2f}')\n",
    "\n",
    "df_test[[f\"gpt3_{target_col}\", target_col, f\"absolute_error_{target_col}\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e431bbd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-22T17:59:33.529327Z",
     "start_time": "2022-10-22T17:59:33.502829Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpt3_grammar</th>\n",
       "      <th>grammar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt3_grammar</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.619356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grammar</th>\n",
       "      <td>0.619356</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              gpt3_grammar   grammar\n",
       "gpt3_grammar      1.000000  0.619356\n",
       "grammar           0.619356  1.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[[\"gpt3_grammar\", \"grammar\"]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa55315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXP1\n",
    "# target_col = \"grammar\"\n",
    "# text_col = \"short_text\"\n",
    "# start_seed = 3222\n",
    "# nb_train_examples_rare_categories = 1\n",
    "# max_length_train_examples = 1500\n",
    "# nb_test_examples_rare_categories = 2\n",
    "## grammar: avg. absolute error: 0.62 +/- 0.68\n",
    "\n",
    "# EXP2\n",
    "# max_length_train_examples = 1000\n",
    "# nb_train_examples_rare_categories = 2\n",
    "#grammar: avg. absolute error: 0.58 +/- 0.67\n",
    "\n",
    "# EXP3\n",
    "# gpt_model_type = \"text-curie-001\"\n",
    "# nb_train_examples_rare_categories = 1\n",
    "# max_length_train_examples = 750\n",
    "# grammar: avg. absolute error: 1.08 +/- 0.73"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ab4a63",
   "metadata": {},
   "source": [
    "## Play around with some prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "23030034",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-20T19:20:57.850283Z",
     "start_time": "2022-10-20T19:20:57.094378Z"
    }
   },
   "outputs": [],
   "source": [
    "# API: https://beta.openai.com/docs/api-reference/completions/create?lang=python\n",
    "output = openai.Completion.create(\n",
    "# ID of the model to use, Required!\n",
    "  model=\"text-davinci-002\",\n",
    "# The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.\n",
    "# Note that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the \n",
    "    # model will generate as if from the beginning of a new document.\n",
    "  prompt=\"Say this is a test\",\n",
    "# The maximum number of tokens to generate in the completion.\n",
    "# The token count of your prompt plus max_tokens cannot exceed the model's context length. \n",
    "# Most models have a context length of 2048 tokens (except for the newest models, which support 4096).\n",
    "  max_tokens=6, \n",
    "# What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, \n",
    "    # and 0 (argmax sampling) for ones with a well-defined answer.\n",
    "# We generally recommend altering this or top_p but not both. Defaults to 1\n",
    "  temperature=0\n",
    "# top_p = 1 --> nucleus sampling,  where the model considers the results of the tokens with top_p probability mass. So 0.1 means \n",
    "    # only the tokens comprising the top 10% probability mass are considered.\n",
    "# We generally recommend altering this or temperature but not both.\n",
    "# n, integer, Optional, Defaults to 1 --> How many completions to generate for each prompt. \n",
    "# Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you \n",
    "    # have reasonable settings for max_tokens and stop.\n",
    "# stream --> stream back partial progress\n",
    "# logprobs --> Include the log probs on the 'logprobs' most likely tokens, max is 5\n",
    "# echo --> echo back the prompt + completion\n",
    "# stop --> Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.\n",
    "# presence_penalty, number, Optional, Defaults to 0 --> Number between -2.0 and 2.0. Positive values penalize new tokens based on whether \n",
    "    # they appear in the text so far, increasing the model's likelihood to talk about new topics.\n",
    "# frequency_penalty, number, Optional, Defaults to 0 --> Number between -2.0 and 2.0. Positive values penalize new tokens based on their \n",
    "    # existing frequency in the text so far, \n",
    "    # decreasing the model's likelihood to repeat the same line verbatim.\n",
    "# Generates best_of completions server-side and returns the \"best\" (the one with the highest log probability per token). Results cannot be streamed. \n",
    "    # When used with n, best_of controls the number of candidate completions and n specifies how many to return – best_of must be greater than n.\n",
    "    # Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have \n",
    "    # reasonable settings for max_tokens and stop.\n",
    "# logit_bias, map, Optional, Defaults to null --> Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens \n",
    "    # (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this tokenizer tool \n",
    "    # (which works for both GPT-2 and GPT-3) to convert text to token IDs. \n",
    "    # Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, \n",
    "    # but values between -1 and 1 should decrease or increase likelihood of selection; \n",
    "    # values like -100 or 100 should result in a ban or exclusive selection of the relevant token. As an example, you can pass {\"50256\": -100} \n",
    "    # to prevent the <|endoftext|> token from being generated.\n",
    "# user, string, Optional --> A unique identifier representing your end-user, which will help OpenAI to monitor and detect abuse.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f9fb9f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-12T20:05:09.681804Z",
     "start_time": "2022-12-12T20:05:09.672948Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"🛰️ I'm leading the machine learning team at Cloud to Street to build cutting edge Deep Learning models for detecting flood water on satellite imagery.\n",
    "\n",
    "🏆 All-time highest rank on Kaggle: #58 worldwide of over 100,000 (https://www.kaggle.com/voltaire).\n",
    "\n",
    "🥇 I scored 1st place in the Open AI Tanzania Challenge for segmentation of building footprints.\n",
    "\n",
    " 🌏 Beyond my career, I’m passionate about finding (and funding) the most effective charities in the world. I’m part of an international\n",
    " community of people who’ve pledged at least 10% of their income over the course of their careers. The Giving What We Can community \n",
    " has over 7,000 members and has pledged in excess of $2.5 billion to the world’s most effective charities. If you’re interested in \n",
    " learning more about the Pledge or if you have any questions, do not hesitate to reach out to me anytime to chat about it.\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6bf1aecc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-20T20:28:04.771919Z",
     "start_time": "2022-10-20T20:27:59.103765Z"
    }
   },
   "outputs": [],
   "source": [
    "output = openai.Completion.create(model=\"text-davinci-002\", prompt=prompt, max_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "aed49181",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-20T20:28:04.798029Z",
     "start_time": "2022-10-20T20:28:04.773672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Details\n",
      "\n",
      "\n",
      "- I am a Machine Learning Engineer with over 7 years of hands on experience. Effortlessly solve challenging problems made easier via the right ML models! I am also an instructor at lambda school where i train students on the deeper aspects of data science.\n",
      "- I made my mark with my participation on Kaggle the world's largest data science community with over 2MM members classifying satellite images into 40 different classes.\n",
      "- The aim isto classification of land use, land\n"
     ]
    }
   ],
   "source": [
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f663bf48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-20T20:28:49.281311Z",
     "start_time": "2022-10-20T20:28:46.140714Z"
    }
   },
   "outputs": [],
   "source": [
    "output = openai.Completion.create(model=\"text-davinci-002\", prompt=prompt, max_tokens=100, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "25bdc609",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-20T20:28:49.307111Z",
     "start_time": "2022-10-20T20:28:49.283202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "## 📚 Education\n",
      "\n",
      "**Master of Science in Data Science**\n",
      "\n",
      "University of California, Berkeley\n",
      "\n",
      "**Bachelor of Science in Mathematics**\n",
      "\n",
      "University of California, Berkeley\n",
      "\n",
      "## 📝 Publications\n",
      "\n",
      "**Detecting Flood Water on Satellite Imagery**\n",
      "\n",
      "https://towardsdatascience.com/detecting-flood-water-on-satellite-imagery-with-deep-learning-e0a8\n"
     ]
    }
   ],
   "source": [
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bb42655",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-20T20:35:40.661077Z",
     "start_time": "2022-10-20T20:35:40.637813Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Brainstorm some ideas combining BioTech and Github:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fa52a97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-20T20:36:18.952758Z",
     "start_time": "2022-10-20T20:36:14.940630Z"
    }
   },
   "outputs": [],
   "source": [
    "output = openai.Completion.create(model=\"text-davinci-002\", prompt=prompt, max_tokens=200, temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e583528e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-20T20:35:45.094320Z",
     "start_time": "2022-10-20T20:35:45.072448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Create a Github repository for open-source software related to biotech research.\n",
      "2. Use Github to manage and share data related to biotech experiments.\n",
      "3. Use Github to develop and share protocols for biotech experiments.\n",
      "4. Collaborate on biotech research using Github issues and pull requests.\n",
      "5. Share results of biotech experiments on Github.\n",
      "6. Use Github to manage references for biotech research.\n",
      "7. Develop a biotech research platform using Github Pages.\n",
      "8.\n"
     ]
    }
   ],
   "source": [
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d769eb11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-20T20:36:18.977671Z",
     "start_time": "2022-10-20T20:36:18.954080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. A BioTech company could use Github to manage and share their code repositories with collaborators.\n",
      "\n",
      "2. A BioTech company could use Github to create an online database of their research findings.\n",
      "\n",
      "3. A BioTech company could use Github to share information about upcoming conferences and events.\n",
      "\n",
      "4. A BioTech company could use Github to post job openings and attract top talent.\n"
     ]
    }
   ],
   "source": [
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57cbb388",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-20T20:37:26.465323Z",
     "start_time": "2022-10-20T20:37:26.445712Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Brainstorm some ideas combining BioTech and software development or version control:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f9f0473",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-20T20:37:30.670900Z",
     "start_time": "2022-10-20T20:37:26.809232Z"
    }
   },
   "outputs": [],
   "source": [
    "output = openai.Completion.create(model=\"text-davinci-002\", prompt=prompt, max_tokens=200, temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07ae295b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-20T20:37:30.695421Z",
     "start_time": "2022-10-20T20:37:30.672363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-A system that tracks and manages different versions of software code used in bioinformatics algorithms\n",
      "-A tool that uses machine learning to develop more efficient bioinformatics software\n",
      "-A software development platform specifically for developing bioinformatics applications\n",
      "-A version control system for tracking and managing different versions of DNA sequences\n",
      "-A tool that uses artificial intelligence to predict which software changes will improve the performance of bioinformatics algorithms\n"
     ]
    }
   ],
   "source": [
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20213529",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-20T20:38:05.276197Z",
     "start_time": "2022-10-20T20:38:01.101433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-A software development company that specializes in creating applications for the biotech industry\n",
      "-A version control system for biotech software development projects\n",
      "-A software development company that specializes in creating applications for the biotech industry\n",
      "-A web application that allows users to track and predict the success of various biotech drugs in development\n"
     ]
    }
   ],
   "source": [
    "output = openai.Completion.create(model=\"text-davinci-002\", prompt=prompt, max_tokens=200, temperature=1)\n",
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00d5d4aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-20T20:39:21.454034Z",
     "start_time": "2022-10-20T20:39:16.265069Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Tell me a funny story of how Alex successfully founded a company that does something nobody thought was needed:\"\n",
    "output = openai.Completion.create(model=\"text-davinci-002\", prompt=prompt, max_tokens=200, temperature=1)\n",
    "\n",
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "030b5144",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T20:01:24.055073Z",
     "start_time": "2022-10-25T20:01:19.528118Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Nicola had just landed her dream job as a product manager at a startup in Heidelberg. She was thrilled to be starting her new role and was eager to get to work. However, when she went to negotiation her start date and travel reimbursements, she realized that the company was very tight on budget. Nicola had to get creative in her negotiation and ended up striking a deal where she would start work two weeks later than originally planned, but in exchange, the company would reimburse her for her travel costs. She was happy with the compromise and excited to start her new job.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me a funny story of how Nicola negotiated on her starting date and travel reimbursements on her new job as a product manager at a startup in Heidelberg:\"\n",
    "output = openai.Completion.create(model=\"text-davinci-002\", prompt=prompt, max_tokens=200, temperature=1)\n",
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dff93113",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T20:02:02.157350Z",
     "start_time": "2022-10-25T20:01:58.956782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Nicola was very excited to start her new job as a product manager at a startup in Heidelberg. However, she was not happy with the company's offer of start date and travel reimbursement. She decided to negotiate with the company and was successful in getting what she wanted.\n"
     ]
    }
   ],
   "source": [
    "output = openai.Completion.create(model=\"text-davinci-002\", prompt=prompt, max_tokens=200, temperature=1)\n",
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4f77bab0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T20:06:15.986929Z",
     "start_time": "2022-10-25T20:06:08.327239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "One day, Nicola and Jonathan were walking around Heidelberg when they saw a very imposing castle on a hill. They decided to explore it and, after a long climb, they finally reached the top. As they were admiring the view, they heard someone coming up the stairs behind them. They turned around to see a very large, angry-looking guard.\n",
      "\n",
      "The guard shouted at them in German, and Nicola and Jonathan had no idea what he was saying. But the tone of his voice was very clear: they were not welcome there. They quickly ran back down the stairs and didn't look back until they were well away from the castle.\n",
      "\n",
      "It was only then that they realized they had accidentally trespassed into a restricted area. They must have been trespassing for quite some time, because the guard had been coming up the stairs to get them for a while. They must have looked completely ridiculous, running away from the castle like that. But it was all very\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me a very funny story of Nicola and Jonathan in Heidelberg:\"\n",
    "output = openai.Completion.create(model=\"text-davinci-002\", prompt=prompt, max_tokens=200, temperature=1)\n",
    "print(output[\"choices\"][0][\"text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
